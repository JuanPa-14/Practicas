{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computer Vision.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqBwTpX+1kISz35E64mqJH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanPablo20001404/Practicas/blob/main/Computer%20Vision%20-%20Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYyFuRfYwWw6"
      },
      "source": [
        "**Computer Vision Cognitive Service**\n",
        "\n",
        "Prueba de entrenamiento del dia 28 de Octubre del 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLY4R4fXwVTj",
        "outputId": "e23ccbff-e50a-431e-da8e-ce7ad7381ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Instalamos el paquete de Microsft Azure Cognitive Service\n",
        "# -q ejecuta la instalación en modo silencioso\n",
        "!pip install --upgrade azure-cognitiveservices-vision-computervision -q"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 92kB 4.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFRnJbCuxpWu"
      },
      "source": [
        "### Importamos las bibliotecas de funciones\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "from array import array\n",
        "import os\n",
        "from PIL import Image\n",
        "import sys\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6McMBxdyHkC"
      },
      "source": [
        "### Auntenticamos el acceso para consumir el recurso\n",
        "endpoint = \"https://fizgon.cognitiveservices.azure.com/\"\n",
        "subscription_key = \"02421427223e40318c8ea2487fa582db\"\n",
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF7xZX6fEoNk",
        "outputId": "f5d63897-8160-4306-9ba6-e0729aed30b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Detect Faces - remote\n",
        "This example detects faces in a remote image, gets their gender and age, \n",
        "and marks them with a bounding box.\n",
        "'''\n",
        "print(\"===== Detect Faces - remote =====\")\n",
        "# Get an image with faces\n",
        "remote_image_url_faces = \"https://e00-elmundo.uecdn.es/assets/multimedia/imagenes/2017/10/30/15093571924104.jpg\"\n",
        "# Select the visual feature(s) you want.\n",
        "remote_image_features = [\"faces\"]\n",
        "# Call the API with remote URL and features\n",
        "detect_faces_results_remote = computervision_client.analyze_image(remote_image_url_faces, remote_image_features)\n",
        "\n",
        "# Print the results with gender, age, and bounding box\n",
        "print(\"Faces in the remote image: \")\n",
        "if (len(detect_faces_results_remote.faces) == 0):\n",
        "    print(\"No faces detected.\")\n",
        "else:\n",
        "    for face in detect_faces_results_remote.faces:\n",
        "        print(\"'{}' of age {} at location {}, {}, {}, {}\".format(face.gender, face.age, \\\n",
        "        face.face_rectangle.left, face.face_rectangle.top, \\\n",
        "        face.face_rectangle.left + face.face_rectangle.width, \\\n",
        "        face.face_rectangle.top + face.face_rectangle.height))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Detect Faces - remote =====\n",
            "Faces in the remote image: \n",
            "'Female' of age 27 at location 265, 291, 393, 419\n",
            "'Female' of age 25 at location 47, 72, 170, 195\n",
            "'Male' of age 31 at location 48, 291, 170, 413\n",
            "'Male' of age 39 at location 489, 71, 609, 191\n",
            "'Male' of age 71 at location 490, 293, 609, 412\n",
            "'Female' of age 30 at location 272, 74, 384, 186\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}